{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA4S5F2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seenu-g/school_of_ai/blob/master/chap6/trial_All.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kH16rnZ7wt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky3f_Odl-7um",
        "colab_type": "text"
      },
      "source": [
        "## Data Transformations\n",
        "\n",
        "We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtssFUKb-jqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train Phase transformations\n",
        "train_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       #transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                      #  transforms.Resize((28, 28)),\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQciFYo2B1mO",
        "colab_type": "text"
      },
      "source": [
        "# Dataset and Creating Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4A84rlfDA23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgldp_3-Dn0c",
        "colab_type": "text"
      },
      "source": [
        "# Dataloader Arguments & Test/Train Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8OLDR79DrHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-QPKCA2MaGM",
        "colab_type": "text"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FXQlB9kH1ov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout_value = 0.03\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv_layer1 = nn.Sequential(                      \n",
        "            nn.Conv2d(in_channels=1, out_channels=12, kernel_size=(3, 3), padding=0, bias=True),#Op_size = 26, RF = 3\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=12),\n",
        "            nn.Dropout2d(dropout_value)\n",
        "        )\n",
        "        self.conv_layer2 = nn.Sequential(  \n",
        "            nn.Conv2d(in_channels=12, out_channels=12, kernel_size=(3, 3), padding=0, bias=True),#Op_size = 24, RF = 5\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=12),\n",
        "            nn.Dropout2d(dropout_value)\n",
        "        )\n",
        "        self.maxpool_layer1 = nn.MaxPool2d(kernel_size=(2,2)) #Op_size = 12, RF = 6\n",
        "        self.conv_layer3 = nn.Sequential(  \n",
        "            nn.Conv2d(in_channels=12, out_channels=14, kernel_size=(3, 3), padding=0, bias=True), #Op_size = 10, RF = 10   \n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=14),\n",
        "            nn.Dropout2d(dropout_value)\n",
        "        )\n",
        "        self.conv_layer4 = nn.Sequential(  \n",
        "            nn.Conv2d(in_channels=14, out_channels=16, kernel_size=(3, 3), padding=0, bias=True), #Op_size = 8, RF = 14\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=16),\n",
        "            nn.Dropout2d(dropout_value) \n",
        "        )\n",
        "        self.conv_layer5 = nn.Sequential(  \n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=True),#Op_size = 6, RF = 18 \n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        )\n",
        "        self.conv_layer6 = nn.Sequential(  \n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=True),#Op_size = 6, RF = 22 \n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        )            \n",
        "        self.avgpool_layer1 = nn.AvgPool2d(kernel_size=6)\n",
        "\n",
        "        self.conv_layer7 = nn.Sequential(  \n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=True)#Op_size = 1, RF = 28\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer1(x)\n",
        "        x = self.conv_layer2(x)\n",
        "        x =  self.maxpool_layer1(x)\n",
        "        x = self.conv_layer3(x)\n",
        "        x = self.conv_layer4(x)\n",
        "        x = self.conv_layer5(x)\n",
        "        x = self.conv_layer6(x)\n",
        "        x = self.avgpool_layer1(x)\n",
        "        x = self.conv_layer7(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3-vp8X9LCWo",
        "colab_type": "text"
      },
      "source": [
        "*italicized text*# Model Params\n",
        "Can't emphasize on how important viewing Model Summary is. \n",
        "Unfortunately, there is no in-built model visualizer, so we have to take external help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5skB97zIJQQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1,28,28))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1__x_SbrL7z3",
        "colab_type": "text"
      },
      "source": [
        "# Training and Testing\n",
        "\n",
        "Looking at logs can be boring, so we'll introduce **tqdm** progressbar to get cooler logs. \n",
        "\n",
        "Let's write train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kONTKWPfQLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L1_Loss(model, data, factor=0.0005):\n",
        "  l1_crit = nn.L1Loss().to(device)\n",
        "  reg_loss = 0\n",
        "  \n",
        "  for param in model.parameters():\n",
        "    zero_vector = torch.rand_like(param) * 0\n",
        "    reg_loss += l1_crit(param, zero_vector)\n",
        "\n",
        "  \n",
        "  return factor * reg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbkF2nN_LYIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch,isL1=False):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    #CAlculate L1 loss\n",
        "    if(isL1==True):\n",
        "      loss += L1_Loss(model,data,factor=0.0005)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "    \n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Epoch={epoch} Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader,test_losses, test_acc, misclassified):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            for i in range(len(pred)):\n",
        "              if pred[i]!= target[i]:\n",
        "                misclassified.append([data[i], pred[i], target[i]])\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drokW8wWODKq",
        "colab_type": "text"
      },
      "source": [
        "# Let's Train and test our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHPPDRRNgA9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_test_loss_acc(test_losses,test_acc):\n",
        "  fig, axs = plt.subplots(1,2,figsize=(15,10))\n",
        "  axs[0].plot(test_losses)\n",
        "  axs[0].set_title(\"Test Loss\")\n",
        "  axs[1].plot(test_acc)\n",
        "  axs[1].set_title(\"Test Accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQttwdJ_gI0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_misclassified(misclassified):\n",
        "  fig = plt.figure(figsize = (10,10))\n",
        "  for i in range(25):\n",
        "        sub = fig.add_subplot(5, 5, i+1)\n",
        "        plt.imshow(misclassified[i][0].cpu().numpy().squeeze(),cmap='gray',interpolation='none')\n",
        "        \n",
        "        sub.set_title(\"Pred={}, Act={}\".format(str(misclassified[i][1].data.cpu().numpy()),str(misclassified[i][2].data.cpu().numpy())))\n",
        "        \n",
        "  plt.tight_layout()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMCFxeAKOB53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def without_L1L2():\n",
        "  model =  Net().to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "  EPOCHS = 1\n",
        "  for epoch in range(EPOCHS):\n",
        "     print(\"EPOCH:\", epoch)\n",
        "     train(model, device, train_loader, optimizer, epoch, False)\n",
        "     test(model, device, test_loader,test_without_L1L2_Loss, test_without_L1L2_acc,misclassifiedwithoutL1L2)\n",
        "  \n",
        "  plot_test_loss_acc(test_without_L1L2_Loss,test_without_L1L2_acc)\n",
        "  plot_misclassified(misclassifiedwithoutL1L2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FAhJ3R2hX2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def with_L1():\n",
        "    \n",
        "  model =  Net().to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "  #scheduler = StepLR(optimizer, step_size=8, gamma=0.1)\n",
        "  EPOCHS = 1\n",
        "  for epoch in range(EPOCHS):\n",
        "      print(\"EPOCH:\", epoch)\n",
        "      train(model, device, train_loader, optimizer, epoch, True)\n",
        "      test(model, device, test_loader,test_with_L1_Loss, test_with_L1_acc,misclassifiedL1)\n",
        "\n",
        "  plot_test_loss_acc(test_with_L1_Loss, test_with_L1_acc)\n",
        "  plot_misclassified(misclassifiedL1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPk10gyfhhHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def with_L2():\n",
        "    \n",
        "  model =  Net().to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.005)\n",
        "\n",
        "  #scheduler = StepLR(optimizer, step_size=8, gamma=0.1)\n",
        "  EPOCHS = 1\n",
        "  for epoch in range(EPOCHS):\n",
        "      print(\"EPOCH:\", epoch)\n",
        "      train(model, device, train_loader, optimizer, epoch, False)\n",
        "      test(model, device, test_loader,test_with_L2_Loss, test_with_L2_acc,misclassifiedL2)\n",
        "\n",
        "  plot_test_loss_acc(test_with_L2_Loss,test_with_L2_acc)\n",
        "  plot_misclassified(misclassifiedL2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKjzq049hm8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def with_L1_L2():\n",
        "    \n",
        "  model =  Net().to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.005)\n",
        "\n",
        "  #scheduler = StepLR(optimizer, step_size=8, gamma=0.1)\n",
        "  EPOCHS = 1\n",
        "  for epoch in range(EPOCHS):\n",
        "      print(\"EPOCH:\", epoch)\n",
        "      train(model, device, train_loader, optimizer, epoch, True)\n",
        "      test(model, device, test_loader,test_with_L1L2_Loss, test_with_L1L2_acc,misclassifiedL1L2)\n",
        "\n",
        "  plot_test_loss_acc(test_with_L1L2_Loss,test_with_L1L2_acc)\n",
        "  plot_misclassified(misclassifiedL1L2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ3rCdp-hwKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_without_L1L2_Loss = []\n",
        "test_without_L1L2_acc = []\n",
        "misclassifiedwithoutL1L2= []\n",
        "\n",
        "test_with_L1_Loss = []\n",
        "test_with_L1_acc = []\n",
        "misclassifiedL1 = []\n",
        "\n",
        "test_with_L2_Loss = []\n",
        "test_with_L2_acc = []\n",
        "misclassifiedL2 = []\n",
        "\n",
        "test_with_L1L2_Loss = []\n",
        "test_with_L1L2_acc = []\n",
        "misclassifiedL1L2 = []\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXcSTci9kGCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "without_L1L2()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhDWF0E-kK-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with_L1()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBAp8F12kMbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with_L2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1zg9V8akQpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with_L1_L2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddTn8WWoiIT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axs = plt.subplots(1,2, figsize=(20,10))\n",
        "axs[0].plot(test_without_L1L2_Loss, label='without L1 and L2')\n",
        "axs[0].plot(test_with_L1_Loss, label='with L1 ')\n",
        "axs[0].plot(test_with_L2_Loss, label='with L2 ')\n",
        "axs[0].plot(test_with_L1L2_Loss, label='with L1 and L2 ')\n",
        "axs[0].set_title(\"Test Loss\")\n",
        "axs[0].legend()\n",
        "axs[1].plot(test_without_L1L2_acc, label='without L1 and L2')\n",
        "axs[1].plot(test_with_L1_acc, label='with L1 ')\n",
        "axs[1].plot(test_with_L2_acc, label='with L2')\n",
        "axs[1].plot(test_with_L1L2_acc, label='with L1 and L2 ')\n",
        "axs[1].set_title(\"Test Accuracy\")\n",
        "axs[1].legend()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}